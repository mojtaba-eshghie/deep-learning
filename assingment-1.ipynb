{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[b'batch_label', b'labels', b'data', b'filenames']\nb'training batch 1 of 5'\n10000\n[ 59  43  50 ... 140  84  72]\n6\n(3072,)\n"
     ]
    }
   ],
   "source": [
    "dict = unpickle('dataset/data_batch_1')\n",
    "print(list(dict.keys()))\n",
    "print(dict[b'batch_label'])\n",
    "print(dict[b'filenames'].__len__())\n",
    "\n",
    "print(dict[b'data'][0])\n",
    "print(dict[b'labels'][0])\n",
    "print(dict[b'data'][0].shape)\n",
    "#print(dict[b'data'][0].resahpe(32, 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape:  (32, 32, 3)\nlabel:  9\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.919844pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.919844\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.919844 \nL 251.565 248.919844 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 225.041719 \nL 244.365 225.041719 \nL 244.365 7.601719 \nL 26.925 7.601719 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pdcb88d422e)\">\n    <image height=\"218\" id=\"imagea921af3e39\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-7.041719\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m7428133bcd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.3225\" xlink:href=\"#m7428133bcd\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(27.14125 239.640156)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"64.2975\" xlink:href=\"#m7428133bcd\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(61.11625 239.640156)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.2725\" xlink:href=\"#m7428133bcd\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(91.91 239.640156)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"132.2475\" xlink:href=\"#m7428133bcd\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(125.885 239.640156)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"166.2225\" xlink:href=\"#m7428133bcd\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(159.86 239.640156)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"200.1975\" xlink:href=\"#m7428133bcd\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(193.835 239.640156)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.1725\" xlink:href=\"#m7428133bcd\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(227.81 239.640156)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m39a0ccd331\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m39a0ccd331\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m39a0ccd331\" y=\"44.974219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 48.773437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m39a0ccd331\" y=\"78.949219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 82.748437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m39a0ccd331\" y=\"112.924219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 116.723437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m39a0ccd331\" y=\"146.899219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m39a0ccd331\" y=\"180.874219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m39a0ccd331\" y=\"214.849219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 30 -->\n      <g transform=\"translate(7.2 218.648437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 225.041719 \nL 26.925 7.601719 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 225.041719 \nL 244.365 7.601719 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 225.041719 \nL 244.365 225.041719 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.601719 \nL 244.365 7.601719 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pdcb88d422e\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.601719\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "\n",
    "def cifar10_plot(data, img_index=1):\n",
    "    im = data[b'data'][img_index, :]\n",
    "\n",
    "    im_r = im[0:1024].reshape(32, 32)\n",
    "    im_g = im[1024:2048].reshape(32, 32)\n",
    "    im_b = im[2048:].reshape(32, 32)\n",
    "\n",
    "    # this is the funciton I was looking for the whole time...\n",
    "    img = np.dstack((im_r, im_g, im_b))\n",
    "\n",
    "    print(\"shape: \", img.shape)\n",
    "    print(\"label: \", data[b'labels'][img_index])\n",
    "    \n",
    "    #print(\"category:\", meta[b'label_names'][data[b'labels'][im_idx]])         \n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "cifar10_plot(dict)"
   ]
  },
  {
   "source": [
    "# Assignment #1 \n",
    "\n",
    "train set: data_batch_1\n",
    "\n",
    "validation set: data_batch_2\n",
    "\n",
    "test set: test_batch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unpickle(file, n=10000):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "    file (str): file address \n",
    "    num (int): number of datapoints in the file\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    import copy \n",
    "\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    \n",
    "    # let's make the Y (which is a K*n matrix of the one-hot representations of the label of each image)\n",
    "    Y = np.zeros((10, n))\n",
    "    zero_list = [0 for i in range(10)]\n",
    "    i = 0\n",
    "    for a in dict[b'labels']:\n",
    "        '''\n",
    "        right = str(bin(a+1)).split('b')[1]\n",
    "        one_hot = np.array(list('0'*(10-len(right))+right))\n",
    "        '''\n",
    "        one_hot = copy.deepcopy(zero_list)\n",
    "        one_hot[int(a)] = 1\n",
    "        Y[:, i] = one_hot\n",
    "        i += 1\n",
    "        \n",
    "        \n",
    "\n",
    "    return dict[b'data'].T, Y, np.array(dict[b'labels'])\n",
    "\n",
    "train_X, train_Y, train_y = unpickle('dataset/data_batch_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data_):\n",
    "    \"\"\"\n",
    "    This is used to normalize the data w.r.t mean and std: (x - mean) / std\n",
    "    \"\"\"\n",
    "    data = np.copy(data_)\n",
    "    data = data.astype(numpy.float32)\n",
    "    shape = data.shape\n",
    "    mean = np.mean(data, 1)\n",
    "    std = np.std(data, 1)\n",
    "\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        data[:, i] = (data[:, i] - mean) / std\n",
    "\n",
    "    return data\n",
    "    \n",
    "n_train_X = normalize(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3072, 10000)"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3072, 10000)"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "n_train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "print(train_X[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-0.89175713\n"
     ]
    }
   ],
   "source": [
    "print(n_train_X[100, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-0.99022263\n"
     ]
    }
   ],
   "source": [
    "print(n_train_X[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "print(train_X[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "131.3316\n",
      "72.79520891816986\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_X, 1)[1])\n",
    "print(np.std(train_X, 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-0.99022263\n"
     ]
    }
   ],
   "source": [
    "print(n_train_X[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 59, 154, 255, ...,  71, 250,  62],\n",
       "       [ 43, 126, 253, ...,  60, 254,  61],\n",
       "       [ 50, 105, 253, ...,  74, 211,  60],\n",
       "       ...,\n",
       "       [140, 139,  83, ...,  68, 215, 130],\n",
       "       [ 84, 142,  83, ...,  69, 255, 130],\n",
       "       [ 72, 144,  84, ...,  68, 254, 131]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchGDNN:\n",
    "    \n",
    "    def __init__(self, k, d):\n",
    "        # k is the number of classes\n",
    "        # d is the data dimention\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "\n",
    "        # initializing the weight matrix\n",
    "        self.W = np.random.normal(0, 0.01, (k, d))\n",
    "        self.b = np.random.normal(0, 0.01, (k, 1))\n",
    "\n",
    "    \n",
    "    def evaluate_classifier(self, X, W, b):\n",
    "        # Each column in X corresponds to 'one' image in this context having the d*n size\n",
    "        # This function returns: k*n, each row contains probability for the specific class\n",
    "\n",
    "        if len(X.shape) >= 2:\n",
    "            second_dimention = X.shape[1]\n",
    "            temp_b = np.array(b * second_dimention)\n",
    "            result = np.dot(W, X) + temp_b\n",
    "        else:\n",
    "            second_dimention = 0\n",
    "            temp_b = b\n",
    "            result = np.dot(W, X).reshape(10, 1) + temp_b\n",
    "        \n",
    "        \n",
    "\n",
    "        P = np.zeros(result.shape).shape\n",
    "        \n",
    "        if second_dimention:\n",
    "            P = np.zeros(result.shape)\n",
    "            for j in range(second_dimention):\n",
    "                P[:, j] = np.exp(result[:, j]) / sum(np.exp(result[:, j]))\n",
    "        else:\n",
    "            P = np.zeros((result.shape[0], 1))\n",
    "            P = (np.exp(result) / sum(np.exp(result))).reshape((self.k, 1))\n",
    "            # just one row of elements in P\n",
    "        \n",
    "        return P\n",
    "    \n",
    "\n",
    "    def compute_cost(self, X, Y, W, b, lambda_):\n",
    "        n = X.shape[1]\n",
    "        r = np.sum(np.square(self.W))\n",
    "        P = self.evaluate_classifier(X, W, b)\n",
    "        l = 0\n",
    "        for i in range(n):\n",
    "            y = Y[:, i]\n",
    "            p = P[:, i]\n",
    "            probability = np.dot(y, p)\n",
    "            \n",
    "            if probability == 1:\n",
    "                print('!!!!!!!!!!!!!!!!!!!+++++++++++++!!!!!!!!!!!!!!!!!!!!')\n",
    "                print('probability==1 detected...')\n",
    "                print('y: \\n{0}\\np:\\n{1}'.format(y, p))\n",
    "\n",
    "            if probability == 0:\n",
    "                \n",
    "                print('!!!!!!!!!!!!!!!!!!!_______!!!!!!!!!!!!!!!!!!!!')\n",
    "                print('probability=0 detected, this is a bad omen...')\n",
    "                print('y is {0}\\n p is: {1}'.format(y, p))\n",
    "                print('!!!!!!!!!!!!!!!!!!!_______!!!!!!!!!!!!!!!!!!!!')\n",
    "                \n",
    "                probability = 1.00e-100\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            l = - np.log(probability)\n",
    "            \n",
    "\n",
    "            if not l:\n",
    "                print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                print('probablity: {}'.format(probability))\n",
    "                print('l: {}'.format(l))\n",
    "                print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            \n",
    "            assert not np.isinf(l)\n",
    "            \n",
    "        J = (l / n) + (lambda_ * r)\n",
    "        \n",
    "        return J\n",
    "    \n",
    "    def accuracy(self, X, y, W, b):\n",
    "        # y is a vector containing the ground truth label numbers (just like train_y)\n",
    "        P = self.evaluate_classifier(X, W, b)\n",
    "        prediction = np.argmax(P, axis=0)\n",
    "        n = X.shape[1]\n",
    "        incorrect = 0\n",
    "        for i in range(n):\n",
    "            if y[i] != prediction[i]:\n",
    "                incorrect += 1\n",
    "        return (n - incorrect) / n\n",
    "    \n",
    "\n",
    "    def compute_grads_num_slow(self, X, Y, P, W, b, lamda_, h=0.01):\n",
    "        \n",
    "\n",
    "        no \t= \tW.shape[0]\n",
    "        d \t= \tX.shape[0]\n",
    "\n",
    "        grad_W = np.zeros(W.shape)\n",
    "        grad_b = np.zeros((no, 1))\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(len(b)):\n",
    "            b_try = np.array(b)\n",
    "            b_try[i] -= h\n",
    "            c1 = self.compute_cost(X, Y, W, b_try, lamda_)\n",
    "\n",
    "            b_try = np.array(b)\n",
    "            b_try[i] += h\n",
    "            c2 = self.compute_cost(X, Y, W, b_try, lamda_)\n",
    "\n",
    "            grad_b[i] = (c2-c1) / (2*h)\n",
    "        \n",
    "\n",
    "        for i in range(W.shape[0]):\n",
    "            print('>> inside grad computation main loop, i: {}'.format(i))\n",
    "            for j in range(W.shape[1]):\n",
    "\n",
    "                W_try = np.array(W)\n",
    "                W_try[i,j] -= h\n",
    "                c1 = self.compute_cost(X, Y, W_try, b, lamda_)\n",
    "\n",
    "                W_try = np.array(W)\n",
    "                W_try[i,j] += h\n",
    "                c2 = self.compute_cost(X, Y, W_try, b, lamda_)\n",
    "\n",
    "                grad_W[i,j] = (c2-c1) / (2*h)\n",
    "\n",
    "                \n",
    "                if np.isnan(grad_W[i,j]):\n",
    "                    print('You are in a bad situation...')\n",
    "                    print(2*h)\n",
    "                    print(c1)\n",
    "                    print(c2)\n",
    "                    print('X: \\n {}'.format(X))\n",
    "                    print('Y: \\n {}'.format(Y))\n",
    "                    print('Is W_try nan: {0}\\nW_try: \\n {1}'.format(np.isnan(np.sum(W_try)), W_try))\n",
    "                    print('Is b nan: {0}\\nb: \\n {1}'.format(np.isnan(np.sum(b)), b))\n",
    "                    print('lambda: {}'.format(lamda_))\n",
    "                    print('#####################')\n",
    "                \n",
    "                \n",
    "\n",
    "        if np.isnan(np.sum(grad_W)):\n",
    "            print('\\n.   ^^^^^^^^^^^^^^^^^^^^ smt bad: ')\n",
    "        \n",
    "        assert not np.isnan(np.sum(grad_W))\n",
    "\n",
    "        return [grad_W, grad_b]\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, X, Y, eta=0.01, lambda_=0, epochs=100, mini_batch_size=100):\n",
    "        # forward pass\n",
    "        # backward pass\n",
    "        # update weight matrix\n",
    "        \n",
    "        # let's do a permutation of the total dataset size!\n",
    "        n = X.shape[1]\n",
    "        d = X.shape[0]\n",
    "        assert n % mini_batch_size == 0\n",
    "\n",
    "        stacked_arrays = np.concatenate((X, Y))\n",
    "        '''\n",
    "        W = copy.deepcopy(self.W)\n",
    "        temp_ar = np.zeros((self.k, n-d))\n",
    "        temp_ar2 = np.concatenate((W, temp_ar),  axis=1)\n",
    "        stacked_arrays = np.concatenate((stacked_arrays, temp_ar2))\n",
    "        '''\n",
    "        \n",
    "        np.take(stacked_arrays, np.random.permutation(n), axis=1, out=stacked_arrays)\n",
    "        \n",
    "        X_, Y_ = np.split(stacked_arrays, [self.d], axis=0)\n",
    "        \n",
    "        '''\n",
    "        Y_, W = np.split(Y_W, [self.k], axis=0)\n",
    "        W = copy.deepcopy(W[:, 0:self.d])\n",
    "        '''\n",
    "        \n",
    "\n",
    "        for i in range(epochs):\n",
    "            '''\n",
    "            # if we use train_y as well:\n",
    "            temp1 = np.concatenate((train_X, train_Y))\n",
    "            temp2 = train_y.reshape((train_y.shape[0], 1)).T\n",
    "            stacked_arrays = np.concatenate((temp1, temp2)).shape\n",
    "            '''\n",
    "            \n",
    "\n",
    "            # remember that you need to stack the newly changed W matrix on this not the old one!!!\n",
    "\n",
    "            \n",
    "\n",
    "            num_of_batches = int(n / mini_batch_size)\n",
    "            l_index = 0\n",
    "            h_index = 0\n",
    "            for i in range(num_of_batches):\n",
    "                l_index = h_index\n",
    "                h_index = (i+1) * mini_batch_size\n",
    "                batch_X = X_[:, l_index:h_index]\n",
    "                batch_Y = Y_[:, l_index:h_index]\n",
    "                P = self.evaluate_classifier(batch_X, self.W, self.b)\n",
    "\n",
    "                grad_W, grad_b = self.compute_grads_num_slow(batch_X, batch_Y, P, self.W, self.b, lambda_)\n",
    "                \n",
    "                print('******** grads computed, batch number: {}'.format(i))\n",
    "                \n",
    "                # TODO: remove this:\n",
    "                '''\n",
    "                print('shape of W: {}'.format(self.W.shape))\n",
    "                print('shape of grad_W: {}'.format(grad_W.shape))\n",
    "                print('shape of b: {}'.format(self.b.shape))\n",
    "                print('shape of grad_b: {}'.format(grad_b.shape))\n",
    "                print('this is grad_W:')\n",
    "                print(grad_W)\n",
    "                print('dtype of grad_W: {0}, dtype of W: {1}'.format(self.W.dtype, grad_W.dtype))\n",
    "                print('eta is: {}'.format(eta))\n",
    "                print('result of operation (temp): ')\n",
    "                print(self.W - (eta * grad_W))\n",
    "                '''\n",
    "\n",
    "                print('dimentions of grad_W: {}'.format(grad_W.shape))\n",
    "\n",
    "                self.W = self.W - (eta * grad_W)\n",
    "                self.b = self.b - (eta * grad_b)\n",
    "\n",
    "                # TODO: remove this\n",
    "                print('------------------------')\n",
    "            \n",
    "\n",
    "            \n",
    "            stacked_arrays = np.concatenate((X_, Y_))\n",
    "            '''\n",
    "            temp_ar2 = np.concatenate((W, temp_ar),  axis=1)\n",
    "            stacked_arrays = np.concatenate((stacked_arrays, temp_ar2))\n",
    "            '''\n",
    "            \n",
    "            np.take(stacked_arrays, np.random.permutation(n), axis=1, out=stacked_arrays)\n",
    "            \n",
    "            X_, Y_ = np.split(stacked_arrays, [self.d], axis=0)\n",
    "            '''\n",
    "            Y_, W = np.split(Y_W, [self.k], axis=0)\n",
    "            W = copy.deepcopy(W[:, 0:self.d])\n",
    "            '''\n",
    "            \n",
    "            # TODO: remove this\n",
    "            print('epoch ended...')\n",
    "\n",
    "        \n",
    "        # When all of epochs end, we have to replace the self.W with the heavily updated W!\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 2)\n(2, 3)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [11, 12,  1,  2,  3]])"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "ar1 = np.array([[1, 2], [11, 12]])\n",
    "ar2 = np.array([[3, 4, 5], [1, 2, 3]])\n",
    "print(ar1.shape)\n",
    "print(ar2.shape)\n",
    "np.concatenate((ar1, ar2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MiniBatchGDNN(k=10, d=3072)\n",
    "#res = nn.evaluate_classifier(train_X[:, 0], nn.W, nn.b)\n",
    "#nn.compute_cost(n_train_X, train_Y, nn.W, nn.b, 0.01)\n",
    "#nn.accuracy(n_train_X, train_y, nn.W, nn.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> inside grad computation main loop, i: 0\n",
      "*** KeyboardInterrupt exception caught in code being profiled."
     ]
    },
    {
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 7.21623 s\n",
      "File: <ipython-input-89-38c923c97c54>\n",
      "Function: train at line 158\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   158                                               def train(self, X, Y, eta=0.01, lambda_=0, epochs=100, mini_batch_size=100):\n",
      "   159                                                   # forward pass\n",
      "   160                                                   # backward pass\n",
      "   161                                                   # update weight matrix\n",
      "   162                                                   \n",
      "   163                                                   # let's do a permutation of the total dataset size!\n",
      "   164         1          4.0      4.0      0.0          n = X.shape[1]\n",
      "   165         1          1.0      1.0      0.0          d = X.shape[0]\n",
      "   166         1          1.0      1.0      0.0          assert n % mini_batch_size == 0\n",
      "   167                                           \n",
      "   168         1     315739.0 315739.0      4.4          stacked_arrays = np.concatenate((X, Y))\n",
      "   169                                                   '''\n",
      "   170                                                   W = copy.deepcopy(self.W)\n",
      "   171                                                   temp_ar = np.zeros((self.k, n-d))\n",
      "   172                                                   temp_ar2 = np.concatenate((W, temp_ar),  axis=1)\n",
      "   173                                                   stacked_arrays = np.concatenate((stacked_arrays, temp_ar2))\n",
      "   174                                                   '''\n",
      "   175                                                   \n",
      "   176         1     218668.0 218668.0      3.0          np.take(stacked_arrays, np.random.permutation(n), axis=1, out=stacked_arrays)\n",
      "   177                                                   \n",
      "   178         1         54.0     54.0      0.0          X_, Y_ = np.split(stacked_arrays, [self.d], axis=0)\n",
      "   179                                                   \n",
      "   180                                                   '''\n",
      "   181                                                   Y_, W = np.split(Y_W, [self.k], axis=0)\n",
      "   182                                                   W = copy.deepcopy(W[:, 0:self.d])\n",
      "   183                                                   '''\n",
      "   184                                                   \n",
      "   185                                           \n",
      "   186         1         31.0     31.0      0.0          for i in range(epochs):\n",
      "   187                                                       '''\n",
      "   188                                                       # if we use train_y as well:\n",
      "   189                                                       temp1 = np.concatenate((train_X, train_Y))\n",
      "   190                                                       temp2 = train_y.reshape((train_y.shape[0], 1)).T\n",
      "   191                                                       stacked_arrays = np.concatenate((temp1, temp2)).shape\n",
      "   192                                                       '''\n",
      "   193                                                       \n",
      "   194                                           \n",
      "   195                                                       # remember that you need to stack the newly changed W matrix on this not the old one!!!\n",
      "   196                                           \n",
      "   197                                                       \n",
      "   198                                           \n",
      "   199         1          2.0      2.0      0.0              num_of_batches = int(n / mini_batch_size)\n",
      "   200         1          1.0      1.0      0.0              l_index = 0\n",
      "   201         1          1.0      1.0      0.0              h_index = 0\n",
      "   202         1          1.0      1.0      0.0              for i in range(num_of_batches):\n",
      "   203         1          1.0      1.0      0.0                  l_index = h_index\n",
      "   204         1          1.0      1.0      0.0                  h_index = (i+1) * mini_batch_size\n",
      "   205         1          1.0      1.0      0.0                  batch_X = X_[:, l_index:h_index]\n",
      "   206         1          1.0      1.0      0.0                  batch_Y = Y_[:, l_index:h_index]\n",
      "   207         1       1617.0   1617.0      0.0                  P = self.evaluate_classifier(batch_X, self.W, self.b)\n",
      "   208                                           \n",
      "   209         1    6680108.0 6680108.0     92.6                  grad_W, grad_b = self.compute_grads_num_slow(batch_X, batch_Y, P, self.W, self.b, lambda_)\n",
      "   210                                                           \n",
      "   211                                                           print('******** grads computed, batch number: {}'.format(i))\n",
      "   212                                                           \n",
      "   213                                                           # TODO: remove this:\n",
      "   214                                                           '''\n",
      "   215                                                           print('shape of W: {}'.format(self.W.shape))\n",
      "   216                                                           print('shape of grad_W: {}'.format(grad_W.shape))\n",
      "   217                                                           print('shape of b: {}'.format(self.b.shape))\n",
      "   218                                                           print('shape of grad_b: {}'.format(grad_b.shape))\n",
      "   219                                                           print('this is grad_W:')\n",
      "   220                                                           print(grad_W)\n",
      "   221                                                           print('dtype of grad_W: {0}, dtype of W: {1}'.format(self.W.dtype, grad_W.dtype))\n",
      "   222                                                           print('eta is: {}'.format(eta))\n",
      "   223                                                           print('result of operation (temp): ')\n",
      "   224                                                           print(self.W - (eta * grad_W))\n",
      "   225                                                           '''\n",
      "   226                                           \n",
      "   227                                                           print('dimentions of grad_W: {}'.format(grad_W.shape))\n",
      "   228                                           \n",
      "   229                                                           self.W = self.W - (eta * grad_W)\n",
      "   230                                                           self.b = self.b - (eta * grad_b)\n",
      "   231                                           \n",
      "   232                                                           # TODO: remove this\n",
      "   233                                                           print('------------------------')\n",
      "   234                                                       \n",
      "   235                                           \n",
      "   236                                                       \n",
      "   237                                                       stacked_arrays = np.concatenate((X_, Y_))\n",
      "   238                                                       '''\n",
      "   239                                                       temp_ar2 = np.concatenate((W, temp_ar),  axis=1)\n",
      "   240                                                       stacked_arrays = np.concatenate((stacked_arrays, temp_ar2))\n",
      "   241                                                       '''\n",
      "   242                                                       \n",
      "   243                                                       np.take(stacked_arrays, np.random.permutation(n), axis=1, out=stacked_arrays)\n",
      "   244                                                       \n",
      "   245                                                       X_, Y_ = np.split(stacked_arrays, [self.d], axis=0)\n",
      "   246                                                       '''\n",
      "   247                                                       Y_, W = np.split(Y_W, [self.k], axis=0)\n",
      "   248                                                       W = copy.deepcopy(W[:, 0:self.d])\n",
      "   249                                                       '''\n",
      "   250                                                       \n",
      "   251                                                       # TODO: remove this\n",
      "   252                                                       print('epoch ended...')"
     ],
     "name": "stdout"
    }
   ],
   "source": [
    "%lprun -f nn.train nn.train(n_train_X[0:3072, :], train_Y, eta=0.01, epochs=10)\n",
    "#%lprun -f nn.train nn.train(n_train_X, train_Y, eta=0.01, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1005"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "nn.accuracy(n_train_X[0:5, :], train_y, nn.W, nn.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "print(nn.W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10, 5)\n(5, 10000)\n(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(nn.W.shape)\n",
    "print(n_train_X[0:5, :].shape)\n",
    "print(train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = nn.compute_cost(n_train_X, train_Y, nn.W, nn.b, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = nn.evaluate_classifier(n_train_X[0:5, :], nn.W, nn.b)f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0974"
      ]
     },
     "metadata": {},
     "execution_count": 859
    }
   ],
   "source": [
    "nn.accuracy(n_train_X[0:5, :], train_y, nn.W, nn.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape of train_X: (3072, 10000)\n[154 126 105 102 125 155 172 180 142 111 106 109 123 127 181 217 209 166\n 164 158 116 102  95  90  72  60  56  77  94  91  87  79 140 145 125 124\n 150 152 174 178 134 110 133 163 192 218 240 245 241 238]\nshape of n_train_X: (3072, 10000)\n[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0\n   0   0   0   0   0   0   0 255 255   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   1   1   1   1   1]\n"
     ]
    }
   ],
   "source": [
    "print('shape of train_X: {}'.format(train_X.shape))\n",
    "print(train_X[0:50, 1])\n",
    "print('shape of n_train_X: {}'.format(train_X.shape))\n",
    "print(n_train_X[0:50, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[132.0083 131.3316 132.1932 132.8545 133.5792 134.3971 134.7978 135.1851\n 135.8141 136.297  136.8169 137.113  137.3297 137.1162 136.8877 136.9053\n 137.1568 137.2577 137.2893 137.2488 136.9061 136.7277 136.4994 136.2089\n 135.7952 135.3322 134.872  134.1757 133.4242 132.8146 132.0122 131.9019\n 131.3094 130.401  131.3778 131.7695 132.2499 132.9564 133.2472 133.5744\n 134.1906 134.9024 135.5159 135.8525 136.1054 135.9043 135.6061 135.7247\n 135.9062 136.0032]\n[73.72932002 72.79520892 72.49635904 72.19688726 72.1032782  71.88391901\n 71.58008323 71.44139443 71.12615511 71.19297712 70.97483339 70.84230538\n 70.8606929  70.76832694 70.68004449 70.64497669 70.67071822 70.71601014\n 70.66743667 70.84215622 71.02103972 71.24657152 71.18513609 71.1998038\n 71.24973022 71.33803784 71.5401343  71.84855064 72.33407395 72.46220551\n 72.91341887 73.23642998 73.15373997 72.2499301  71.87634567 71.49677034\n 71.32137583 70.93864884 70.55254561 70.4199039  70.13925486 69.91123282\n 69.60276681 69.55933973 69.50880729 69.42870114 69.39420684 69.29244338\n 69.20658207 69.257053  ]\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(train_X, 1)\n",
    "std = np.std(train_X, 1)\n",
    "print(mean[0:50])\n",
    "print(std[0:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.29827675, -0.07324158, -0.3750974 , -0.42736548, -0.11898452,\n",
       "        0.28661346,  0.5197285 ,  0.6272956 ,  0.08697087, -0.3553301 ,\n",
       "       -0.43419436, -0.39683917, -0.20222324, -0.14294833,  0.62411153,\n",
       "        1.1337601 ,  1.01659   ,  0.40644646,  0.3779774 ,  0.29292175,\n",
       "       -0.29436454, -0.4874307 , -0.5829697 , -0.6490021 , -0.89537627,\n",
       "       -1.0559981 , -1.1024871 , -0.7957794 , -0.54502976, -0.5770546 ,\n",
       "       -0.6173357 , -0.7223444 ,  0.11879887,  0.2020635 , -0.08873308,\n",
       "       -0.10866959,  0.24887115,  0.2684516 ,  0.57762367,  0.63086724,\n",
       "       -0.00271741, -0.35620022, -0.03614654,  0.39027867,  0.80413663,\n",
       "        1.1824437 ,  1.5043609 ,  1.577017  ,  1.5185467 ,  1.4727528 ],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "n_train_X[0:50, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.copy(train_X)\n",
    "b = b.astype(numpy.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 59. 154. 255. ...  71. 250.  62.]\n [ 43. 126. 253. ...  60. 254.  61.]\n [ 50. 105. 253. ...  74. 211.  60.]\n ...\n [140. 139.  83. ...  68. 215. 130.]\n [ 84. 142.  83. ...  69. 255. 130.]\n [ 72. 144.  84. ...  68. 254. 131.]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}